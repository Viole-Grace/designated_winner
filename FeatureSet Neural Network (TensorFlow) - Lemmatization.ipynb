{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_lem = open('sentiment_set_lem_5000.pickle','rb')\n",
    "x_lem_tr, x_lem_te, y_lem_tr, y_lem_te = pickle.load(pickle_lem)\n",
    "pickle_lem.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sizes :  9596 1066 9596 1066\n",
      "Shape of input data :  424 1066\n"
     ]
    }
   ],
   "source": [
    "print \"Sizes : \",len(x_lem_tr), len(x_lem_te), len(y_lem_tr), len(y_lem_te)\n",
    "print \"Shape of input data : \", len(x_lem_tr[0]), len(y_lem_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_stem = open('sentiment_set_stem.pickle','rb')\n",
    "# x_stem_tr, x_stem_te, y_stem_tr, y_stem_te = pickle.load(pickle_stem)\n",
    "# pickle_stem.close()\n",
    "# print \"Sizes : \",len(x_stem_tr), len(x_stem_te), len(y_stem_tr), len(y_stem_te)\n",
    "# print \"Shape : \", len(x_stem_tr[0]), len(x_stem_te[0]), (y_stem_tr[0]), (y_stem_te[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define network params\n",
    "node_size, classes = [768, 512, 256], 2\n",
    "batch_size = 128\n",
    "\n",
    "X,y = tf.placeholder('float', [None, len(x_lem_tr[0])]), tf.placeholder('float')\n",
    "def neural_network(data):\n",
    "    hidden_1 = {'weights':tf.Variable(tf.random_normal([len(x_lem_tr[0]), node_size[0]])),\n",
    "                'biases':tf.Variable(tf.random_normal([node_size[0]]))}\n",
    "    hidden_2 = {'weights':tf.Variable(tf.random_normal([node_size[0], node_size[1]])),\n",
    "                'biases':tf.Variable(tf.random_normal([node_size[1]]))}\n",
    "    hidden_3 = {'weights':tf.Variable(tf.random_normal([node_size[1], node_size[2]])),\n",
    "                'biases':tf.Variable(tf.random_normal([node_size[2]]))}\n",
    "    output_l = {'weights':tf.Variable(tf.random_normal([node_size[2], classes])),\n",
    "                'biases':tf.Variable(tf.random_normal([classes]))}\n",
    "    \n",
    "    l1 = tf.add(tf.matmul(data,hidden_1['weights']), hidden_1['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    \n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2['weights']), hidden_2['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    \n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3['weights']), hidden_3['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    output = tf.add(tf.matmul(l3,output_l['weights']), output_l['biases'])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def train(x):\n",
    "    \n",
    "    prediction = neural_network(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    epochs = 100\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss, i = 0, 0            \n",
    "            while i < len(x_lem_tr):\n",
    "                if i+batch_size < len(x_lem_tr):\n",
    "                    epoch_x = np.array(x_lem_tr[i:i+batch_size]) #moving window batch\n",
    "                    epoch_y = np.array(y_lem_tr[i:i+batch_size])\n",
    "                    i=i+batch_size\n",
    "#                     print \"Current batch : [\",(i-batch_size),\"~\",i,\"]\"\n",
    "                else:\n",
    "                    epoch_x = np.array(x_lem_tr[i:]) #till the last element\n",
    "                    epoch_y = np.array(y_lem_tr[i:])\n",
    "#                     print \"Current batch : [\",i,\"~\",len(x_lem_tr),\"]\"\n",
    "                    i = len(x_lem_tr)\n",
    "                _,c = sess.run([optimizer, cost], feed_dict={x:epoch_x, y:epoch_y})\n",
    "                epoch_loss += c\n",
    "            print \"*****************************************\\n\"\n",
    "            print \"Epoch : \",epoch+1,\" of \",epochs,\" epochs\"\n",
    "            print \"Loss : \",epoch_loss\n",
    "        correct = tf.equal(tf.argmax(prediction, -1), tf.argmax(y, -1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        try :\n",
    "            print \"Accuracy : \", accuracy.eval({x:x_lem_te, y:y_lem_te})\n",
    "        except Exception as e:\n",
    "            print \"Failed to calc accuracy : not in range\"\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "\n",
      "Epoch :  1  of  100  epochs\n",
      "Loss :  156960.90826416016\n",
      "*****************************************\n",
      "\n",
      "Epoch :  2  of  100  epochs\n",
      "Loss :  79935.75244140625\n",
      "*****************************************\n",
      "\n",
      "Epoch :  3  of  100  epochs\n",
      "Loss :  50334.92892456055\n",
      "*****************************************\n",
      "\n",
      "Epoch :  4  of  100  epochs\n",
      "Loss :  32338.86293029785\n",
      "*****************************************\n",
      "\n",
      "Epoch :  5  of  100  epochs\n",
      "Loss :  21273.38591003418\n",
      "*****************************************\n",
      "\n",
      "Epoch :  6  of  100  epochs\n",
      "Loss :  15032.399063110352\n",
      "*****************************************\n",
      "\n",
      "Epoch :  7  of  100  epochs\n",
      "Loss :  17478.715827941895\n",
      "*****************************************\n",
      "\n",
      "Epoch :  8  of  100  epochs\n",
      "Loss :  14673.351440429688\n",
      "*****************************************\n",
      "\n",
      "Epoch :  9  of  100  epochs\n",
      "Loss :  7355.977582931519\n",
      "*****************************************\n",
      "\n",
      "Epoch :  10  of  100  epochs\n",
      "Loss :  7004.338896751404\n",
      "*****************************************\n",
      "\n",
      "Epoch :  11  of  100  epochs\n",
      "Loss :  5799.419863700867\n",
      "*****************************************\n",
      "\n",
      "Epoch :  12  of  100  epochs\n",
      "Loss :  3578.9852571487427\n",
      "*****************************************\n",
      "\n",
      "Epoch :  13  of  100  epochs\n",
      "Loss :  2285.83038854599\n",
      "*****************************************\n",
      "\n",
      "Epoch :  14  of  100  epochs\n",
      "Loss :  1250.444796860218\n",
      "*****************************************\n",
      "\n",
      "Epoch :  15  of  100  epochs\n",
      "Loss :  636.2452937960625\n",
      "*****************************************\n",
      "\n",
      "Epoch :  16  of  100  epochs\n",
      "Loss :  474.4584504783179\n",
      "*****************************************\n",
      "\n",
      "Epoch :  17  of  100  epochs\n",
      "Loss :  471.941457554698\n",
      "*****************************************\n",
      "\n",
      "Epoch :  18  of  100  epochs\n",
      "Loss :  441.6250967979431\n",
      "*****************************************\n",
      "\n",
      "Epoch :  19  of  100  epochs\n",
      "Loss :  374.10624915361404\n",
      "*****************************************\n",
      "\n",
      "Epoch :  20  of  100  epochs\n",
      "Loss :  360.55758798122406\n",
      "*****************************************\n",
      "\n",
      "Epoch :  21  of  100  epochs\n",
      "Loss :  355.579619333148\n",
      "*****************************************\n",
      "\n",
      "Epoch :  22  of  100  epochs\n",
      "Loss :  402.7033340521157\n",
      "*****************************************\n",
      "\n",
      "Epoch :  23  of  100  epochs\n",
      "Loss :  442.38438323140144\n",
      "*****************************************\n",
      "\n",
      "Epoch :  24  of  100  epochs\n",
      "Loss :  448.27348770946264\n",
      "*****************************************\n",
      "\n",
      "Epoch :  25  of  100  epochs\n",
      "Loss :  422.6969266217202\n",
      "*****************************************\n",
      "\n",
      "Epoch :  26  of  100  epochs\n",
      "Loss :  453.1256181150675\n",
      "*****************************************\n",
      "\n",
      "Epoch :  27  of  100  epochs\n",
      "Loss :  501.5930211544037\n",
      "*****************************************\n",
      "\n",
      "Epoch :  28  of  100  epochs\n",
      "Loss :  493.3971932530403\n",
      "*****************************************\n",
      "\n",
      "Epoch :  29  of  100  epochs\n",
      "Loss :  566.2982850074768\n",
      "*****************************************\n",
      "\n",
      "Epoch :  30  of  100  epochs\n",
      "Loss :  577.5706394538283\n",
      "*****************************************\n",
      "\n",
      "Epoch :  31  of  100  epochs\n",
      "Loss :  564.2816817313433\n",
      "*****************************************\n",
      "\n",
      "Epoch :  32  of  100  epochs\n",
      "Loss :  529.4813531935215\n",
      "*****************************************\n",
      "\n",
      "Epoch :  33  of  100  epochs\n",
      "Loss :  619.9918241351843\n",
      "*****************************************\n",
      "\n",
      "Epoch :  34  of  100  epochs\n",
      "Loss :  561.4859974347055\n",
      "*****************************************\n",
      "\n",
      "Epoch :  35  of  100  epochs\n",
      "Loss :  551.5095526874065\n",
      "*****************************************\n",
      "\n",
      "Epoch :  36  of  100  epochs\n",
      "Loss :  647.866779923439\n",
      "*****************************************\n",
      "\n",
      "Epoch :  37  of  100  epochs\n",
      "Loss :  723.7075762748718\n",
      "*****************************************\n",
      "\n",
      "Epoch :  38  of  100  epochs\n",
      "Loss :  657.1385583877563\n",
      "*****************************************\n",
      "\n",
      "Epoch :  39  of  100  epochs\n",
      "Loss :  698.1139540672302\n",
      "*****************************************\n",
      "\n",
      "Epoch :  40  of  100  epochs\n",
      "Loss :  621.0497792959213\n",
      "*****************************************\n",
      "\n",
      "Epoch :  41  of  100  epochs\n",
      "Loss :  705.4167873859406\n",
      "*****************************************\n",
      "\n",
      "Epoch :  42  of  100  epochs\n",
      "Loss :  905.7781057991087\n",
      "*****************************************\n",
      "\n",
      "Epoch :  43  of  100  epochs\n",
      "Loss :  873.5156463384628\n",
      "*****************************************\n",
      "\n",
      "Epoch :  44  of  100  epochs\n",
      "Loss :  789.9569918513298\n",
      "*****************************************\n",
      "\n",
      "Epoch :  45  of  100  epochs\n",
      "Loss :  920.5692662000656\n",
      "*****************************************\n",
      "\n",
      "Epoch :  46  of  100  epochs\n",
      "Loss :  934.2266097068787\n",
      "*****************************************\n",
      "\n",
      "Epoch :  47  of  100  epochs\n",
      "Loss :  852.2317403554916\n",
      "*****************************************\n",
      "\n",
      "Epoch :  48  of  100  epochs\n",
      "Loss :  914.4138643741608\n",
      "*****************************************\n",
      "\n",
      "Epoch :  49  of  100  epochs\n",
      "Loss :  874.8371942937374\n",
      "*****************************************\n",
      "\n",
      "Epoch :  50  of  100  epochs\n",
      "Loss :  833.5068635940552\n",
      "*****************************************\n",
      "\n",
      "Epoch :  51  of  100  epochs\n",
      "Loss :  1123.4534873962402\n",
      "*****************************************\n",
      "\n",
      "Epoch :  52  of  100  epochs\n",
      "Loss :  1042.202442407608\n",
      "*****************************************\n",
      "\n",
      "Epoch :  53  of  100  epochs\n",
      "Loss :  1079.8440616130829\n",
      "*****************************************\n",
      "\n",
      "Epoch :  54  of  100  epochs\n",
      "Loss :  865.413430929184\n",
      "*****************************************\n",
      "\n",
      "Epoch :  55  of  100  epochs\n",
      "Loss :  882.6823396682739\n",
      "*****************************************\n",
      "\n",
      "Epoch :  56  of  100  epochs\n",
      "Loss :  948.9563524648547\n",
      "*****************************************\n",
      "\n",
      "Epoch :  57  of  100  epochs\n",
      "Loss :  956.8083939552307\n",
      "*****************************************\n",
      "\n",
      "Epoch :  58  of  100  epochs\n",
      "Loss :  938.3262255191803\n",
      "*****************************************\n",
      "\n",
      "Epoch :  59  of  100  epochs\n",
      "Loss :  1019.6893270015717\n",
      "*****************************************\n",
      "\n",
      "Epoch :  60  of  100  epochs\n",
      "Loss :  940.7336201071739\n",
      "*****************************************\n",
      "\n",
      "Epoch :  61  of  100  epochs\n",
      "Loss :  843.7556593418121\n",
      "*****************************************\n",
      "\n",
      "Epoch :  62  of  100  epochs\n",
      "Loss :  972.1577873267233\n",
      "*****************************************\n",
      "\n",
      "Epoch :  63  of  100  epochs\n",
      "Loss :  971.4688720703125\n",
      "*****************************************\n",
      "\n",
      "Epoch :  64  of  100  epochs\n",
      "Loss :  902.8668736219406\n",
      "*****************************************\n",
      "\n",
      "Epoch :  65  of  100  epochs\n",
      "Loss :  1044.3634848594666\n",
      "*****************************************\n",
      "\n",
      "Epoch :  66  of  100  epochs\n",
      "Loss :  912.0002315044403\n",
      "*****************************************\n",
      "\n",
      "Epoch :  67  of  100  epochs\n",
      "Loss :  923.7816071510315\n",
      "*****************************************\n",
      "\n",
      "Epoch :  68  of  100  epochs\n",
      "Loss :  763.9479997716844\n",
      "*****************************************\n",
      "\n",
      "Epoch :  69  of  100  epochs\n",
      "Loss :  930.4411688446999\n",
      "*****************************************\n",
      "\n",
      "Epoch :  70  of  100  epochs\n",
      "Loss :  845.8065000474453\n",
      "*****************************************\n",
      "\n",
      "Epoch :  71  of  100  epochs\n",
      "Loss :  767.9731452465057\n",
      "*****************************************\n",
      "\n",
      "Epoch :  72  of  100  epochs\n",
      "Loss :  860.2669574394822\n",
      "*****************************************\n",
      "\n",
      "Epoch :  73  of  100  epochs\n",
      "Loss :  882.787488758564\n",
      "*****************************************\n",
      "\n",
      "Epoch :  74  of  100  epochs\n",
      "Loss :  1113.0498082637787\n",
      "*****************************************\n",
      "\n",
      "Epoch :  75  of  100  epochs\n",
      "Loss :  1201.0440120697021\n",
      "*****************************************\n",
      "\n",
      "Epoch :  76  of  100  epochs\n",
      "Loss :  1180.5837802886963\n",
      "*****************************************\n",
      "\n",
      "Epoch :  77  of  100  epochs\n",
      "Loss :  1127.498142004013\n",
      "*****************************************\n",
      "\n",
      "Epoch :  78  of  100  epochs\n",
      "Loss :  882.0299067497253\n",
      "*****************************************\n",
      "\n",
      "Epoch :  79  of  100  epochs\n",
      "Loss :  1258.1535859107971\n",
      "*****************************************\n",
      "\n",
      "Epoch :  80  of  100  epochs\n",
      "Loss :  1023.2289417386055\n",
      "*****************************************\n",
      "\n",
      "Epoch :  81  of  100  epochs\n",
      "Loss :  1196.986790895462\n",
      "*****************************************\n",
      "\n",
      "Epoch :  82  of  100  epochs\n",
      "Loss :  1161.2203948497772\n",
      "*****************************************\n",
      "\n",
      "Epoch :  83  of  100  epochs\n",
      "Loss :  1158.504878282547\n",
      "*****************************************\n",
      "\n",
      "Epoch :  84  of  100  epochs\n",
      "Loss :  1185.2647438943386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "\n",
      "Epoch :  85  of  100  epochs\n",
      "Loss :  989.3317996436763\n",
      "*****************************************\n",
      "\n",
      "Epoch :  86  of  100  epochs\n",
      "Loss :  923.8683874607086\n",
      "*****************************************\n",
      "\n",
      "Epoch :  87  of  100  epochs\n",
      "Loss :  808.8209829181433\n",
      "*****************************************\n",
      "\n",
      "Epoch :  88  of  100  epochs\n",
      "Loss :  899.0379943847656\n",
      "*****************************************\n",
      "\n",
      "Epoch :  89  of  100  epochs\n",
      "Loss :  669.3905050754547\n",
      "*****************************************\n",
      "\n",
      "Epoch :  90  of  100  epochs\n",
      "Loss :  755.3439790010452\n",
      "*****************************************\n",
      "\n",
      "Epoch :  91  of  100  epochs\n",
      "Loss :  868.425080910325\n",
      "*****************************************\n",
      "\n",
      "Epoch :  92  of  100  epochs\n",
      "Loss :  811.9386837780476\n",
      "*****************************************\n",
      "\n",
      "Epoch :  93  of  100  epochs\n",
      "Loss :  804.333838313818\n",
      "*****************************************\n",
      "\n",
      "Epoch :  94  of  100  epochs\n",
      "Loss :  854.2129392623901\n",
      "*****************************************\n",
      "\n",
      "Epoch :  95  of  100  epochs\n",
      "Loss :  794.6070215702057\n",
      "*****************************************\n",
      "\n",
      "Epoch :  96  of  100  epochs\n",
      "Loss :  747.0855446942151\n",
      "*****************************************\n",
      "\n",
      "Epoch :  97  of  100  epochs\n",
      "Loss :  846.460112541914\n",
      "*****************************************\n",
      "\n",
      "Epoch :  98  of  100  epochs\n",
      "Loss :  847.1516190767288\n",
      "*****************************************\n",
      "\n",
      "Epoch :  99  of  100  epochs\n",
      "Loss :  667.755048433115\n",
      "*****************************************\n",
      "\n",
      "Epoch :  100  of  100  epochs\n",
      "Loss :  696.0444153062999\n",
      "Accuracy :  0.6022514\n"
     ]
    }
   ],
   "source": [
    "train(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
